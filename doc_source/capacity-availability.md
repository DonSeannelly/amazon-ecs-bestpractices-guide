# Capacity and availability<a name="capacity-availability"></a>

Application availability is crucial for providing an error\-free experience and for minimizing application latency\. Availability depends on having resources that are accessible and have enough capacity to meet demand\. AWS provides several mechanisms to manage availability\. For applications hosted on Amazon ECS, these include autoscaling and Availability Zones \(AZs\)\. Autoscaling manages the number of tasks or instances based on metrics you define, while Availability Zones allow you to host your application in isolated but geographically\-close locations\.

As with task sizes, capacity and availability present certain trade\-offs you must consider\. Ideally, capacity would be perfectly aligned with demand\. There would always be just enough capacity to serve requests and process jobs to meet Service Level Objectives \(SLOs\) including a low latency and error rate\. Capacity would never be too high, leading to excessive cost; nor would it never be too low, leading to high latency and error rates\.

Autoscaling is a latent process\. First, real\-time metrics must be delivered to CloudWatch\. They must then be aggregated for analysis, which can take up to several minutes depending on the granularity of the metric\. Then, CloudWatch compares the metrics against alarm thresholds to identify a shortage or excess of resources\. To prevent instability, alarms are usually configured to require the threshold be crossed for a few minutes before firing\. Finally, it takes time to provision new tasks and to terminate tasks that are no longer needed\.

Because of these delays, it is important to maintain some headroom by overprovisioning to accommodate short\-term bursts in demand\. This will allow your application to service additional requests without becoming saturated\. A good general rule is to set a scaling target between 60\-80% of utilization\. This will allow your application to handle bursts of extra demand while additional capacity is being provisioned\.

Another reason to overprovision is to quickly respond to Availability Zone failures\. AWS recommends that production workloads be served from multiple Availability Zones\. Should an Availability Zone failure occur, tasks running in the remaining Availability Zones will serve demand\. If your application runs in two Availability Zones, then you will need to double your normal task count in order to provide immediate capacity during a failure\. If your application runs in three Availability Zones, then you should run 1\.5 times your normal task count \(three tasks for every two that are needed for ordinary serving\)\.

## Maximizing scaling speed<a name="capacity-availability-speed"></a>

Autoscaling is a reactive process that takes time to take effect\. There are some ways to help minimize time to scale out\.

**Minimize image size\.** Larger images take longer to download from an image repository and unpack\. Keeping image sizes to a minimum can reduce the amount of time it takes for a container to start\.
+ If you are able to build a static binary or are using Golang, you may be able to build your image `FROM` scratch and include only your binary application in the resulting image\.
+ Consider using minimized base images from upstream distro vendors such as Amazon Linux or Ubuntu\.
+ Don’t include any build artifacts in your final image\. Using multi\-stage builds can help with this\.
+ Compact `RUN` stages wherever possible\. Each `RUN` stage creates a new image layer, leading to an additional round trip to download the layer\. A single `RUN` stage that has multiple commands joined by `&&` will have fewer layers than one with multiple `RUN` stages\.
+ If you must include data,such as ML inference data, in your final image, include only the data that's needed to start up and begin serving traffic\. If it is possible to fetch data on demand from Amazon S3 or other storage without impacting service, do that instead\.

**Keep your images close\.** The higher the network latency, the longer it will take to download the image\. Host your images in a repository in the same AWS Region that your workload is in\. Amazon ECR is a high\-performance image repository that is available in every AWS Region that Amazon ECS is available in\. Avoid traversing the Internet or a VPN link to download container images\. Hosting your images in the same Region improves overall reliability by mitigating the risk of network connectivity issues, or of availability issues in a different Region\. Amazon ECR cross\-region replication can help with this\.

**Reduce load balancer health check thresholds\.** Load balancers perform health checks before sending traffic to your application\. The default health check configuration for a target group can take 90 seconds or longer to marking your task as healthy and capable of receiving requests\. Lowering the health check interval and threshold count can make your application take traffic sooner and reduce load on other tasks\.

**Consider cold\-start performance\.** Some application runtimes such as Java perform Just\-In\-Time \(JIT\) compilation, which can cause an application to perform slowly at first, then run faster as the compilation process finishes\. Consider rewriting latency\-critical parts of your workload in languages that do not impose a cold\-start performance penalty\.

**Use step scaling instead of target\-tracking scaling policies\.** You have several Application Auto Scaling options for Amazon ECS tasks\. Target tracking is the easiest mode to use: you simply set a target value for a metric \(such as CPU average utilization\), and the auto scaler will automatically manage the number of tasks needed to attain that value\. With step scaling, you define the specific thresholds for your scaling metrics, and how many tasks to add or remove when the thresholds are crossed\. Step scaling is more challenging to configure, but offers you more fine\-grained control: you can react very quickly to changes in demand by minimizing the amount of time a threshold alarm is in breach\. For more information, see [Service Auto Scaling](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html) in the *Amazon Elastic Container Service Developer Guide*\.

If you are using Amazon EC2 instances to provide cluster capacity, consider the following recommendations:

**Use larger Amazon EC2 instances and faster Amazon EBS volumes\.** You can maximize image download and preparation speed by using a larger Amazon EC2 instance and faster Amazon EBS volume\. Within a given Amazon EC2 instance family, network and Amazon EBS maximum throughput increases as the instance size increase, for example from `m5.xlarge` to `m5.2xlarge`\. Additionally, Amazon EBS volumes can be customized to increase throughput and IOPS\. If you’re using `gp2` volumes, larger volumes offer more baseline throughput\. If you're using `gp3` volumes, you can specify throughput and IOPS when you create the volume\.

**Use bridge network mode for tasks running on Amazon EC2 instances\.** Tasks that use `bridge` network mode on Amazon EC2 start faster than tasks that use the `awsvpc` network mode\. When `awsvpc` network mode is used, Amazon ECS attaches an elastic network interface \(ENI\) to the instance before launching the task\. This introduces additional latency\. There are several tradeoffs for using bridge networking though\. These tasks don't get their own security group, and there are some implications for load balancing\. For more information, see [Load balancer target groups](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html) in the *Elastic Load Balancing User Guide*\.

## Handling demand shocks<a name="capacity-availability-shocks"></a>

Some applications experience sudden large shocks in demand\. An example would be a news event, a big sale, a media event, or any other event that causes traffic to quickly and significantly increase in a very short amount of time\. If unplanned, this can cause demand to quickly outstrip available resources\.

The best way to handle demand shocks is to anticipate them and plan accordingly\. Since autoscaling can take time, your goal should be to scale out your application before the demand shock happens\. This may require changes to your business process\. For example, if an important event is going to take place at a certain time, the team planning the event should inform the team that manages application scaling with enough time to prepare\. The application team can then configure scheduled scaling to scale out the service before the event begins, and scale it in again once the event ends\. A shared calendar can be helpful\. For more information, see [Scheduled scaling](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html) in the *Application Auto Scaling User Guide*\.

If you have an Enterprise Support plan, be sure to work with your Technical Account Manager \(TAM\)\. Your TAM can help you prepare by verifying your service quotas and ensuring that any necessary quotas are raised before the event begins, so that you don't accidentally hit any service limits\. They can also help you by prewarming services such as load balancers to ensure your event goes smoothly\.

Handling unscheduled demand shocks is a more difficult problem\. Unscheduled shocks, if large enough in amplitude, can quickly cause demand to outstrip capacity, and outpace the ability for autoscaling to react\. The best way to prepare for unscheduled shocks is to overprovision resources\. Enough resources must be available to handle maximum anticipated traffic demand at any time\.

Maintaining maximum capacity in anticipation of unscheduled demand shocks can be costly\. To mitigate the cost impact, try to find a leading indicator metric or event that predicts a large demand shock is imminent\. If the metric or event reliably provides significant advance notice, begin the scale\-out process immediately when the event occurs or when the metric crosses the threshold value\.

If your application is prone to sudden unscheduled demand shocks, consider adding a high performance mode to your application that sacrifices non\-critical functionality but retains crucial functionality for a customer\. For example, if your application can switch from generating expensive customized responses to serving a static response page, you may be able to increase throughput significantly without scaling the application at all\.

You may also consider breaking apart monolithic services\. If your application is a monolithic service that is expensive to run and slow to scale, it may be possible to extract or rewrite performance\-critical pieces and run them as separate services\. The new, separate services then can be scaled independently from less\-critical components\. Having the flexibility to scale out performance\-critical functionality separately from other parts of your application can both reduce the time it takes to add capacity and help conserve cost\.